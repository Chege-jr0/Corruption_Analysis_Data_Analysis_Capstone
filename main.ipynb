{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d16325f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wbdata\n",
    "import pandas as pd\n",
    "import os \n",
    "from google.cloud import bigquery\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f71a0ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your service account JSON (from earlier setup)\n",
    "SERVICE_ACCOUNT_PATH ='data-analytics-capstone-476406-c4809696b27e.json-credentials.json'  # Update this path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "920f90b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate the json file\n",
    "client = bigquery.Client.from_service_account_json(\"data-analytics-capstone-476406-c4809696b27e.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6cc3c73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define parameters\n",
    "# Step 2: Define parameters \n",
    "project_id = 'data-analytics-capstone-476406'\n",
    "dataset_id = 'corruption_analysis'\n",
    "african_countries = [\n",
    "    'DZA', 'AGO', 'BEN', 'BWA', 'BFA', 'BDI', 'CPV', 'CMR', 'CAF', 'TCD',  # Algeria to Chad\n",
    "    'COM', 'COD', 'COG', 'CIV', 'DJI', 'EGY', 'GNQ', 'ERI', 'SWZ', 'ETH',  # Comoros to Ethiopia\n",
    "    'GAB', 'GMB', 'GHA', 'GIN', 'GNB', 'KEN', 'LSO', 'LBR', 'LBY', 'MDG',  # Gabon to Madagascar\n",
    "    'MWI', 'MLI', 'MRT', 'MUS', 'MAR', 'MOZ', 'NAM', 'NER', 'NGA',         # Malawi to Nigeria\n",
    "    'RWA', 'STP', 'SEN', 'SYC', 'SLE', 'SOM', 'ZAF', 'SSD', 'SDN',         # Rwanda to Sudan\n",
    "    'TZA', 'TGO', 'TUN', 'UGA', 'ZMB', 'ZWE'                               # Tanzania to Zimbabwe\n",
    "]\n",
    "start_year = 2015\n",
    "end_year = 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ef16d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the indicators \n",
    "indicators = {\n",
    "    'NY.GDP.MKTP.KD.ZG': 'gdp_growth',  # GDP growth (annual %)\n",
    "    'NY.GDP.PCAP.CD': 'gdp_per_capita',  # GDP per capita (current US$)\n",
    "    'BX.KLT.DINV.CD.WD': 'fdi_inflow'  # Foreign direct investment, net inflows (BoP, current US$)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd5ac4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching gdp_growth (NY.GDP.MKTP.KD.ZG) across 3 batches...\n",
      "  Batch 1/3: 20 countries (e.g., ['DZA', 'AGO', 'BEN']...)\n",
      "  Batch 1 returned 180 raw items\n",
      "  Batch 2/3: 20 countries (e.g., ['GAB', 'GMB', 'GHA']...)\n",
      "  Batch 2 returned 180 raw items\n",
      "  Batch 3/3: 14 countries (e.g., ['STP', 'SEN', 'SYC']...)\n",
      "  Batch 3 returned 126 raw items\n",
      "\n",
      "Fetching gdp_per_capita (NY.GDP.PCAP.CD) across 3 batches...\n",
      "  Batch 1/3: 20 countries (e.g., ['DZA', 'AGO', 'BEN']...)\n",
      "  Batch 1 returned 180 raw items\n",
      "  Batch 2/3: 20 countries (e.g., ['GAB', 'GMB', 'GHA']...)\n",
      "  Batch 2 returned 180 raw items\n",
      "  Batch 3/3: 14 countries (e.g., ['STP', 'SEN', 'SYC']...)\n",
      "  Batch 3 returned 126 raw items\n",
      "\n",
      "Fetching fdi_inflow (BX.KLT.DINV.CD.WD) across 3 batches...\n",
      "  Batch 1/3: 20 countries (e.g., ['DZA', 'AGO', 'BEN']...)\n",
      "  Batch 1 returned 180 raw items\n",
      "  Batch 2/3: 20 countries (e.g., ['GAB', 'GMB', 'GHA']...)\n",
      "  Batch 2 returned 180 raw items\n",
      "  Batch 3/3: 14 countries (e.g., ['STP', 'SEN', 'SYC']...)\n",
      "  Batch 3 returned 126 raw items\n",
      "\n",
      "Fetched 1417 rows across 3 indicators for 54 countries (2015-2023).\n",
      "\n",
      "Summary by indicator:\n",
      "                count          mean           min           max\n",
      "indicator_name                                                 \n",
      "fdi_inflow        480  9.335479e+08 -7.397295e+09  4.065879e+10\n",
      "gdp_growth        469  3.060000e+00 -2.946000e+01  3.249000e+01\n",
      "gdp_per_capita    468  2.615530e+03  1.920700e+02  1.914151e+04\n",
      "\n",
      "Sample data:\n",
      "   country country_code  year indicator_name     value\n",
      "0   Angola          AGO  2023     gdp_growth  1.078100\n",
      "1   Angola          AGO  2022     gdp_growth  3.044727\n",
      "2   Angola          AGO  2021     gdp_growth  1.199211\n",
      "3   Angola          AGO  2020     gdp_growth -5.638215\n",
      "4   Angola          AGO  2019     gdp_growth -0.702273\n",
      "5   Angola          AGO  2018     gdp_growth -1.316362\n",
      "6   Angola          AGO  2017     gdp_growth -0.147213\n",
      "7   Angola          AGO  2016     gdp_growth -2.580050\n",
      "8   Angola          AGO  2015     gdp_growth  0.943572\n",
      "9  Burundi          BDI  2023     gdp_growth  2.665540\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Fetch data from World Bank API (with batching for large country lists)\n",
    "import math\n",
    "\n",
    "\n",
    "all_data = []\n",
    "date_range = f\"{start_year}:{end_year}\"\n",
    "batch_size = 20  # Safe chunk size; adjust to 15-25 if needed\n",
    "num_batches = math.ceil(len(african_countries) / batch_size)  # e.g., 54 / 20 = 3 batches\n",
    "\n",
    "for indicator_code, indicator_name in indicators.items():\n",
    "    print(f\"\\nFetching {indicator_name} ({indicator_code}) across {num_batches} batches...\")\n",
    "    \n",
    "    for batch_num in range(num_batches):\n",
    "        start_idx = batch_num * batch_size\n",
    "        batch_countries = african_countries[start_idx:start_idx + batch_size]\n",
    "        countries_str = ';'.join(batch_countries)\n",
    "        \n",
    "        url = f\"https://api.worldbank.org/v2/country/{countries_str}/indicator/{indicator_code}?format=json&date={date_range}&per_page=5000\"\n",
    "        print(f\"  Batch {batch_num + 1}/{num_batches}: {len(batch_countries)} countries (e.g., {batch_countries[:3]}...)\")\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"  Error in batch {batch_num + 1}: {response.status_code} - {response.text[:200]}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            api_response = response.json()\n",
    "            data = api_response[1] if len(api_response) > 1 else []  # Safely grab data array\n",
    "            print(f\"  Batch {batch_num + 1} returned {len(data)} raw items\")\n",
    "        except (IndexError, ValueError) as e:\n",
    "            print(f\"  JSON parse error in batch {batch_num + 1}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        for item in data:\n",
    "            value = item.get('value')\n",
    "            if value is not None:\n",
    "                try:\n",
    "                    value = float(value)\n",
    "                    country_code = item.get('countryiso3code', 'Unknown')\n",
    "                    all_data.append({\n",
    "                        'country': item['country']['value'],\n",
    "                        'country_code': country_code,\n",
    "                        'year': int(item['date']),\n",
    "                        'indicator_name': indicator_name,\n",
    "                        'value': value\n",
    "                    })\n",
    "                except (ValueError, KeyError) as e:\n",
    "                    print(f\"  Skipping invalid row in batch {batch_num + 1}: {e}\")\n",
    "                    continue\n",
    "\n",
    "# Rest of the code (DataFrame creation and prints) stays the same\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "# Safely handle the case when no data (or columns) were fetched to avoid KeyError\n",
    "if df.empty:\n",
    "    print(\"No data fetched. Please check the API responses, parameters, or network connectivity.\")\n",
    "else:\n",
    "    total_indicators = df['indicator_name'].nunique() if 'indicator_name' in df.columns else 0\n",
    "    total_countries = df['country'].nunique() if 'country' in df.columns else len(african_countries)\n",
    "    print(f\"\\nFetched {len(df)} rows across {total_indicators} indicators for {total_countries} countries ({start_year}-{end_year}).\")\n",
    "\n",
    "    # Summary by indicator only if the necessary columns exist\n",
    "    if 'indicator_name' in df.columns and 'value' in df.columns:\n",
    "        print(\"\\nSummary by indicator:\")\n",
    "        print(df.groupby('indicator_name')['value'].agg(['count', 'mean', 'min', 'max']).round(2))\n",
    "\n",
    "    print(\"\\nSample data:\")\n",
    "    print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5d49bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading 1417 rows to data-analytics-capstone-476406.corruption_analysis.economic_indicators...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\DATA ANALYTICS\\GENDATA\\Data_Analytics_Capstone_Project\\venv\\Lib\\site-packages\\google\\cloud\\bigquery\\_pandas_helpers.py:484: FutureWarning: Loading pandas DataFrame into BigQuery will require pandas-gbq package version 0.26.1 or greater in the future. Tried to import pandas-gbq and got: No module named 'pandas_gbq'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully loaded 1417 rows to data-analytics-capstone-476406.corruption_analysis.economic_indicators.\n",
      "   Inferred schema: [('country', 'STRING'), ('country_code', 'STRING'), ('year', 'INTEGER'), ('indicator_name', 'STRING'), ('value', 'FLOAT')]\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Load to BigQuery\n",
    "table_id = f\"{project_id}.{dataset_id}.economic_indicators\"\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition='WRITE_TRUNCATE',  # Overwrite table if it exists (or use 'WRITE_APPEND' to add rows)\n",
    "    autodetect=True  # Automatically infer schema from df (e.g., value as FLOAT64)\n",
    ")\n",
    "\n",
    "print(f\"\\nLoading {len(df)} rows to {table_id}...\")\n",
    "\n",
    "job = client.load_table_from_dataframe(df, table_id, job_config=job_config)\n",
    "job.result()  # Block and wait for the job to complete\n",
    "\n",
    "if job.errors:\n",
    "    print(\"Load errors encountered:\")\n",
    "    for error in job.errors:\n",
    "        print(f\"  - {error}\")\n",
    "else:\n",
    "    print(f\"✅ Successfully loaded {job.output_rows} rows to {table_id}.\")\n",
    "    \n",
    "    # Fixed: Fetch the table post-load to get schema reliably\n",
    "    table = client.get_table(table_id)\n",
    "    print(f\"   Inferred schema: {[(field.name, field.field_type) for field in table.schema]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
